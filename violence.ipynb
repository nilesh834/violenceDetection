{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmDDMU0EGxVW",
        "outputId": "d26cca15-3784-42de-b32a-e3318d138289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/keras-team/keras-preprocessing.git\n",
            "  Cloning https://github.com/keras-team/keras-preprocessing.git to /tmp/pip-req-build-fbcytdvw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/keras-team/keras-preprocessing.git /tmp/pip-req-build-fbcytdvw\n",
            "  Resolved https://github.com/keras-team/keras-preprocessing.git to commit 3e380065d4afc7347aaee8d89325a16b22158438\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing==1.1.2) (1.22.4)\n",
            "Building wheels for collected packages: Keras-Preprocessing\n",
            "  Building wheel for Keras-Preprocessing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Keras-Preprocessing: filename=Keras_Preprocessing-1.1.2-py3-none-any.whl size=43617 sha256=3c580b87a31e8ca869261b3845cb80c784179eef497c4266dd4503642fd2edfd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7hbubino/wheels/72/0b/c7/3f6b26f2d789c712867e02502e1f1b740091f8615f32a9f870\n",
            "Successfully built Keras-Preprocessing\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/keras-team/keras-preprocessing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMmDcnvB_-si",
        "outputId": "8a01fa57-3c1c-45cb-f3bf-9abe7e1c5b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rst9nIMkASCo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YipfFFMmBYyO"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyHAOKCdCIbR"
      },
      "outputs": [],
      "source": [
        "training_set = '/content/drive/MyDrive/DATASET/training_set'\n",
        "test_set = '/content/drive/MyDrive/DATASET/test_set'\n",
        "prediction = '/content/drive/MyDrive/DATASET/prediction'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgoJvGzzKKq3"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rMycwXADyZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8f395d-d1e2-4680-f0e9-2907008d813a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 39123 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        training_set,\n",
        "        target_size= (224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqWl2CC8E0UN",
        "outputId": "a5ef2974-0907-44e0-a931-178e5170eccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17255 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        test_set,\n",
        "        target_size=(224,224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bW6xpfDAMeI"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "training_set_labels = to_categorical(training_set.labels)\n",
        "test_set_labels = to_categorical(test_set.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcNg8ZizFrp7"
      },
      "outputs": [],
      "source": [
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntfMYFE7Fw0A"
      },
      "outputs": [],
      "source": [
        "img_size = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd82LeaMF2SG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10dc110-6e72-46a7-a3da-7c8a51bdaa35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = tf.keras.applications.MobileNet(input_shape=(224, 224, 3),\n",
        "                                             include_top=False,\n",
        "                                             weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcRb93n7F7qu"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLzN-BipGFJN"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLOR6-Sv5ulz",
        "outputId": "c33a211c-0bd8-40b3-e7e6-4b388f8e24b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1223/1223 [==============================] - 12486s 10s/step - loss: 0.2517 - accuracy: 0.9621 - val_loss: 0.1962 - val_accuracy: 0.9618\n",
            "Epoch 2/10\n",
            "1223/1223 [==============================] - 670s 548ms/step - loss: 0.1082 - accuracy: 0.9896 - val_loss: 0.1083 - val_accuracy: 0.9790\n",
            "Epoch 3/10\n",
            "1223/1223 [==============================] - 670s 548ms/step - loss: 0.0614 - accuracy: 0.9928 - val_loss: 0.0824 - val_accuracy: 0.9816\n",
            "Epoch 4/10\n",
            "1223/1223 [==============================] - 671s 549ms/step - loss: 0.0390 - accuracy: 0.9949 - val_loss: 0.0851 - val_accuracy: 0.9770\n",
            "Epoch 5/10\n",
            "1223/1223 [==============================] - 670s 548ms/step - loss: 0.0279 - accuracy: 0.9956 - val_loss: 0.0814 - val_accuracy: 0.9752\n",
            "Epoch 6/10\n",
            "1223/1223 [==============================] - 671s 549ms/step - loss: 0.0198 - accuracy: 0.9967 - val_loss: 0.0862 - val_accuracy: 0.9707\n",
            "Epoch 7/10\n",
            "1223/1223 [==============================] - 670s 548ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.0795 - val_accuracy: 0.9789\n",
            "Epoch 8/10\n",
            "1223/1223 [==============================] - 670s 548ms/step - loss: 0.0134 - accuracy: 0.9973 - val_loss: 0.1299 - val_accuracy: 0.9642\n",
            "Epoch 9/10\n",
            "1223/1223 [==============================] - 672s 549ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 0.1713 - val_accuracy: 0.9541\n",
            "Epoch 10/10\n",
            "1223/1223 [==============================] - 670s 548ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.0942 - val_accuracy: 0.9732\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(training_set,\n",
        "                    epochs=10,\n",
        "                    validation_data=test_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/DATASET/violence_detection_model.h5')"
      ],
      "metadata": {
        "id": "82y9NNXy8PAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WITHOUT KEY FRAME EXTRACTION\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load saved model\n",
        "model = load_model('/content/drive/MyDrive/DATASET/violence_detection_model.h5')\n",
        "\n",
        "# Define function to classify frame as violent or non-violent using model\n",
        "def classify_frame(frame, model):\n",
        "    img = cv2.resize(frame, (224, 224))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    pred = model.predict(img)\n",
        "    if pred[0][0] > 0.5:\n",
        "        return 'violence'\n",
        "    else:\n",
        "        return 'non-violence'\n",
        "\n",
        "# Define input video path and output directories for violent and non-violent frames\n",
        "input_video_path = '/content/drive/MyDrive/DATASET/prediction/input/hospital.mp4'\n",
        "violence_dir = '/content/drive/MyDrive/DATASET/prediction/violence/frames0.1'\n",
        "non_violence_dir = '/content/drive/MyDrive/DATASET/prediction/non-violence/frames0.1'\n",
        "\n",
        "# Open video file\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_count += 1\n",
        "\n",
        "    # Classify frame and save in appropriate directory\n",
        "    classification = classify_frame(frame, model)\n",
        "    frame_name = f\"frame_{frame_count:05d}.jpg\"\n",
        "    if classification == 'violence':\n",
        "        cv2.imwrite(os.path.join(violence_dir, frame_name), frame)\n",
        "    else:\n",
        "        cv2.imwrite(os.path.join(non_violence_dir, frame_name), frame)\n",
        "\n",
        "# Release video file and print statistics\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(f\"Total frames processed: {frame_count}\")\n",
        "print(f\"Violent frames saved: {len(os.listdir(violence_dir))}\")\n",
        "print(f\"Non-violent frames saved: {len(os.listdir(non_violence_dir))}\")\n"
      ],
      "metadata": {
        "id": "T26R8PMAWXAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUVXUu1eWfks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WITH KEY FRAME EXTRACTION\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load saved model\n",
        "model = load_model('/content/drive/MyDrive/DATASET/violence_detection_model.h5')\n",
        "\n",
        "# Define function to classify frame as violent or non-violent using model\n",
        "def classify_frame(frame, model):\n",
        "    img = cv2.resize(frame, (224, 224))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    pred = model.predict(img)\n",
        "    if pred[0][0] > 0.5:\n",
        "        return 'violence'\n",
        "    else:\n",
        "        return 'non-violence'\n",
        "\n",
        "# Define input video path and key frame interval\n",
        "input_video_path = '/content/drive/MyDrive/DATASET/prediction/input/V_313.mp4'\n",
        "key_frame_interval = 10\n",
        "\n",
        "# Define output directories for violent and non-violent frames\n",
        "violence_dir = '/content/drive/MyDrive/DATASET/prediction/violence/frame000'\n",
        "non_violence_dir = '/content/drive/MyDrive/DATASET/prediction/non-violence/frame000'\n",
        "\n",
        "# Open video file\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "frame_count = 0\n",
        "prev_key_frame = None\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_count += 1\n",
        "    if frame_count == 1:\n",
        "        prev_key_frame = frame\n",
        "        key_frame = frame\n",
        "    elif frame_count % key_frame_interval == 0:\n",
        "        mae = np.mean(cv2.absdiff(frame, prev_key_frame))\n",
        "        if mae > 0.05:\n",
        "            prev_key_frame = frame\n",
        "            key_frame = frame\n",
        "        else:\n",
        "            continue\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    # Classify key frame and save in appropriate directory\n",
        "    classification = classify_frame(key_frame, model)\n",
        "    frame_name = f\"key_frame_{frame_count:05d}.jpg\"\n",
        "    if classification == 'violence':\n",
        "        cv2.imwrite(os.path.join(violence_dir, frame_name), key_frame)\n",
        "    else:\n",
        "        cv2.imwrite(os.path.join(non_violence_dir, frame_name), key_frame)\n",
        "\n",
        "# Release video file and print statistics\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(f\"Total frames processed: {frame_count}\")\n",
        "print(f\"Violent frames saved: {len(os.listdir(violence_dir))}\")\n",
        "print(f\"Non-violent frames saved: {len(os.listdir(non_violence_dir))}\")\n"
      ],
      "metadata": {
        "id": "Tcpx-BUaWgAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb599a7e-275c-4563-d37a-5aa8013e5561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 416ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Total frames processed: 156\n",
            "Violent frames saved: 16\n",
            "Non-violent frames saved: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOnkxV5I-edf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}